{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AutoEncoder.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNNOcL5AiAoQZ1PZ0PxolF3"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"DZOkgG44G1d0","colab_type":"code","outputId":"060636ed-6e5d-4f49-c0a6-8be60eb7112b","executionInfo":{"status":"ok","timestamp":1586313700425,"user_tz":-540,"elapsed":715,"user":{"displayName":"後藤拓也","photoUrl":"","userId":"13365501340226528906"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Zh_6-tM-HHcR","colab_type":"code","outputId":"a13e9ab1-56e3-4e29-e3ec-e5a74d79e583","executionInfo":{"status":"ok","timestamp":1586313704829,"user_tz":-540,"elapsed":936,"user":{"displayName":"後藤拓也","photoUrl":"","userId":"13365501340226528906"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd \"/content/drive/My Drive/Colab Notebooks/春休み課題\""],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/春休み課題\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Mxcav38zHIr_","colab_type":"code","outputId":"68c5c599-f8ad-43fd-bad5-e4a9eb7378b7","executionInfo":{"status":"ok","timestamp":1586313706165,"user_tz":-540,"elapsed":663,"user":{"displayName":"後藤拓也","photoUrl":"","userId":"13365501340226528906"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%pwd"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/My Drive/Colab Notebooks/春休み課題'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"sAXjdwuGIzB_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"51cba279-efa3-49f5-86c5-6675070252c2","executionInfo":{"status":"ok","timestamp":1586313711586,"user_tz":-540,"elapsed":4905,"user":{"displayName":"後藤拓也","photoUrl":"","userId":"13365501340226528906"}}},"source":["%tensorflow_version 1.x\n","import tensorflow as tf\n","import numpy as np"],"execution_count":4,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oSmDb4hhbkBi","colab_type":"text"},"source":["**Optimizerとは**\n","\n","> tenosrflowのOptimizerは, 重み(w)やバイアス(b)などの変数を更新してくれる. そして, その更新の方法には, 確率的勾配降下法や最急降下法などさまざまな種類が用意されている. cf) オプティマイザの種類と特徴:\n","https://qiita.com/cnloni/items/ad7dcb7521b936d9fc18\n","\n","> 変数(Variable)の値更新には, tf.assignが有効であるが, Optimizerを使えば, その必要もない!! Optimizerを使わずに, 自力で勾配降下法を実装することももちろんできる. cf)カスタムトレーニング:\n","https://www.tensorflow.org/tutorials/customization/custom_training?hl=ja\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YYB0AdllRGxn","colab_type":"text"},"source":["**バッチ学習**\n","\n","> データ(サンプル)を1つずつネットワークに学習する(これを**オンライン学習**(ただし, ランダム選択に限る)ともいう)のではなく, 複数のデータを入力するやり方が, **バッチ学習**である. バッチ学習には, すべてのデータを一度に使う方法と, 分割して行う方法の2つがあり, 分割する方法を**ミニバッチ学習**と(これをバッチ学習とも)いう.\n","\n","\n","> ミニバッチ学習は, 大きすぎるデータを分割することで, データ順序による影響を受けにくいし, 学習の停滞(局所解に陥ってしまうこと)が起きにくい(バッチ学習よりもデータ数が少なく, パラメータの変化に対応しやすい.).\n","\n","> 10000件のデータを1000ずつのサブセットに分割させる場合, **バッチサイズ**は1000となる. また, バッチサイズのことを\"ミニバッチサイズ\"ともいう.\n","\n","> オンライン学習も学習の停滞(局所解に陥ること)が起こりにくい一方, 学習の結果が不安定になりやすい(1つ1つのデータに対してパラメータの更新をするため).\n","cf) https://to-kei.net/neural-network/sgd/\n","\n","> ちなみに\"**イテレーション数**\"とは, データセットに含まれるデータが少なくとも1回は学習に用いられるのに必要な学習回数であり, バッチサイズによって決まる.10000件のデータを1000ずつに分けた場合, イテレーション数は, 10(=10000/1000)となる.\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Kp2NBRaaSO9w","colab_type":"code","colab":{}},"source":["def get_batch(X, size):  #バッチ訓練\n","  a = np.random.choice(len(X), size, replace=False)\n","  return X[a]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vXNiws4_Hjc9","colab_type":"text"},"source":["AutoEncoderのクラス宣言"]},{"cell_type":"code","metadata":{"id":"GmbaOijjHVqX","colab_type":"code","colab":{}},"source":["class Autoencoder:\n","\n","  #変数の初期化\n","  def __init__(self, input_dim, hidden_dim, epoch=250, learning_rate=0.001):\n","    self.epoch = epoch\n","    self.learning_rate = learning_rate\n","\n","    x = tf.placeholder(dtype=tf.float32, shape=[None, input_dim]) #入力層のデータセット\n","\n","    with tf.name_scope('encode'):\n","      weights = tf.Variable(tf.random_normal([input_dim, hidden_dim], dtype=tf.float32), name='weights')  #正規分布に従う乱数\n","      biases = tf.Variable(tf.zeros([hidden_dim]), name='biases')\n","      encoded = tf.nn.tanh(tf.matmul(x, weights) + biases)  #通常のy=wx+bに活性化関数を通して非線形性の担保\n","\n","    with tf.name_scope('decode'):\n","      weights = tf.Variable(tf.random_normal([hidden_dim, input_dim], dtype=tf.float32), name='weights')  \n","      biases = tf.Variable(tf.zeros([input_dim]), name='biases')     #ここね! decode部で用意すべきバイアス項(b)は,input_dim\n","      decoded = tf.matmul(encoded,weights) + biases   #decode部は,そのままのy=wx+b\n","\n","    self.x = x\n","    self.encoded = encoded\n","    self.decoded = decoded\n","\n","    self.loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(self.x, self.decoded)))) #誤差関数\n","    self.train_op = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss) #オプティマイザの選択\n","\n","    self.saver = tf.train.Saver()  #学習中の各パラメータを保存\n","\n","\n","  #def train(self, data):   #(データセットを訓練)\n","  def train(self, data, batch_size=10):  #バッチ訓練の適用(元データをbatch_size=10ずつに分ける)\n","    self.batch_size = batch_size\n","    #num_samples = len(data)\n","    with tf.Session() as sess:\n","      sess.run(tf.global_variables_initializer())  #変数の初期化\n","      for i in range(self.epoch):\n","        #for j in range(num_samples):\n","        for j in range(500):      #バッチ反復回数(十分大きめで！)\n","          batch_data = get_batch(data, self.batch_size)  #バッチデータの取得\n","          #l, _ = sess.run([self.loss, self.train_op], feed_dict={self.x: [data[j]]}) #誤差関数を求めて最適化する\n","          l, _ = sess.run([self.loss, self.train_op], feed_dict={self.x: batch_data}) #バッチ処理をするので!\n","        if i % 10 == 0:\n","          print('epoch {0}: loss = {1}'.format(i, l))  #このprintの仕方ね!\n","          self.saver.save(sess, './model.ckpt')  #学習したパラメータをファイルに保存\n","\n","\n","  def test(self, data):   #新しいデータセットで訓練\n","    with tf.Session() as sess:\n","      self.saver.restore(sess, './model.ckpt')  #学習したパラメータを読みこむ\n","      hidden, reconstructed = sess.run([self.encoded, self.decoded], feed_dict={self.x: data}) #エンコード&デコード処理\n","\n","      print('input', data)      \n","      print('compressed', hidden)\n","      print('reconstructed', reconstructed)\n","      return reconstructed"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ziwtmn_gILdE","colab_type":"code","outputId":"e7a81403-c3da-4f1c-ac5e-d72dd98cdbd9","executionInfo":{"status":"ok","timestamp":1586324686565,"user_tz":-540,"elapsed":912,"user":{"displayName":"後藤拓也","photoUrl":"","userId":"13365501340226528906"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["'''\n","** AutoEncoderのmain実行内容 **\n","from sklearn import datasets\n","\n","hidden_dim = 1\n","data = datasets.load_iris().data\n","input_dim = len(data[0])\n","ae = Autoencoder(input_dim, hidden_dim)\n","ae.train(data)\n","ae.test([[8,4,6,2]])\n","'''"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nfrom sklearn import datasets\\n\\nhidden_dim = 1\\ndata = datasets.load_iris().data\\ninput_dim = len(data[0])\\nae = Autoencoder(input_dim, hidden_dim)\\nae.train(data)\\nae.test([[8,4,6,2]])\\n'"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"oMIDzThjpoip","colab_type":"code","colab":{}},"source":["'''\n","もともと, CIFAR-10.ipynbにあった内容(現在は使えず...)\n","from AutoEncoder import Autoencoder\n","\n","x = np.matrix(data)\n","y = np.array(labels)\n","\n","horse_indices = np.where(y == 7)[0]\n","horse_x = x[horse_indices]\n","print(np.shape(horse_x))\n","\n","input_dim = np.shape(horse_x)[1]\n","hidden_dim = 100\n","ae = Autoencoder(input_dim, hidden_dim)\n","ae.train(horse_x)\n","'''"],"execution_count":0,"outputs":[]}]}