{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DCGAN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNdLMRaeRByAsZ+xVUxr2xe"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"XoYVKJaSk2NL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b249ae84-cb46-4a9f-87e2-671e211a981f","executionInfo":{"status":"ok","timestamp":1580461177009,"user_tz":-540,"elapsed":772,"user":{"displayName":"後藤拓也","photoUrl":"","userId":"13365501340226528906"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"trZog1KMk3iC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"8fc37f14-bd53-4cdb-a3b9-c8a5eb2ee040","executionInfo":{"status":"ok","timestamp":1580461178667,"user_tz":-540,"elapsed":621,"user":{"displayName":"後藤拓也","photoUrl":"","userId":"13365501340226528906"}}},"source":["%cd \"/content/drive/My Drive/Colab Notebooks/slGan\""],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/slGan\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dON3s41klB8-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"1595a658-b090-4039-d1c7-f8e9fa2dd2bf","executionInfo":{"status":"ok","timestamp":1580461180610,"user_tz":-540,"elapsed":946,"user":{"displayName":"後藤拓也","photoUrl":"","userId":"13365501340226528906"}}},"source":["%pwd"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/My Drive/Colab Notebooks/slGan'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"gg01HQzglGAV","colab_type":"code","colab":{}},"source":["%tensorflow_version 1.x\n","import tensorflow as tf\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XJjq1UFa4aP5","colab_type":"code","colab":{}},"source":["!jupyter nbconvert --to python 'Generator'.ipynb\n","!jupyter nbconvert --to python 'Discriminator'.ipynb\n","import Generator as gen\n","import Discriminator as dis"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rZAPJCdYlJUO","colab_type":"code","colab":{}},"source":["class DCGAN():\n","    def __init__(self):\n","        self.G_is_train = tf.placeholder(tf.bool)\n","        self.D_is_train = tf.placeholder(tf.bool)\n","        self.input_X = tf.placeholder(tf.float32, shape=(None, 28 * 28))\n","\n","        # t0は0のラベルを格納し、t1は1のラベルを格納する\n","        self.label_t0 = tf.placeholder(tf.float32, shape=(None, 1))\n","        self.label_t1 = tf.placeholder(tf.float32, shape=(None, 1))\n","\n","        # Generator\n","        self.generator = gen.Generator(device_name=self.device_name)\n","        # 生成モデルに必要なノイズの入れ物\n","        self.gen_z = tf.placeholder(tf.float32, shape=(None, 100))\n","        # Discrimitor\n","        self.discrimitor = dis.Discrimitor(device_name=self.device_name)\n","\n","         # weight decay\n","        gen_norm_term = tf.nn.l2_loss(self.generator.gen_w2) + tf.nn.l2_loss(self.generator.gen_w3)\n","        gen_lambda_ = 0.001\n","\n","        dis_norm_term = tf.nn.l2_loss(self.discrimitor.dis_w2) + tf.nn.l2_loss(self.discrimitor.dis_w3)\n","        dis_lambda_ = 0.001\n","\n","        # 訓練データの識別予測\n","        input_X = self.discrimitor.run(\n","                  self.input_X,\n","                  is_train=self.D_is_train,\n","                  device_name=self.device_name)\n","        \n","        # 生成されたデータの識別予測\n","        generated_X = self.discrimitor.run(\n","            self.generator.run(\n","                z=self.gen_z,\n","                is_train=self.G_is_train,\n","                device_name=self.device_name),\n","            is_train=self.D_is_train,\n","            device_name=self.device_name)\n","        \n","        self.dis_entropy_X = tf.nn.sigmoid_cross_entropy_with_logits(\n","            labels=self.label_t1, logits=input_X)\n","        self.dis_entropy_G = tf.nn.sigmoid_cross_entropy_with_logits(\n","            labels=self.label_t0, logits=generated_X)\n","\n","        self.dis_loss = tf.reduce_mean(\n","            self.dis_entropy_X + self.dis_entropy_G\n","        ) + dis_norm_term * dis_lambda_\n","\n","        self.gen_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n","            labels=self.label_t1, logits=generated_X)\n","        self.gen_loss = tf.reduce_mean(\n","            self.gen_entropy) #+ gen_norm_term * gen_lambda_\n","\n","        # 最適化する際にDならDのみのパラメータを、GならGのみのパラメータを更新するようにしたいのでモデル別の変数を取得する\n","        dis_vars = [\n","            x for x in tf.trainable_variables() if \"dis_\" in x.name\n","        ]\n","        gen_vars = [\n","            x for x in tf.trainable_variables() if \"gen_\" in x.name\n","        ]\n","\n","        # 識別モデルDの最適化\n","        self.opt_d = tf.train.AdamOptimizer(0.0002,beta1=0.1).minimize(\n","                self.dis_loss, var_list=[dis_vars])\n","        # 生成モデルGの最適化\n","        self.opt_g = tf.train.AdamOptimizer(0.0002, beta1=0.5).minimize(\n","                self.gen_loss, var_list=[gen_vars])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"coQ2h_2unBBD","colab_type":"code","colab":{}},"source":["def train(self,\n","          X_train=None,\n","          batch_size=100,\n","          epoch_num=1000,\n","          imgpath='./mnist_DCGAN_images/',\n","          ckptpath='./mnist_DCGAN_checkpoints/',\n","          log_file='mnist_DCGAN_loss_log.csv',\n","          init=False):\n","\n","    if X_train is None:\n","        raise TypeError(\"X_train is None\")\n","\n","    # 訓練途中で生成データを作成して保存したいのでその保存先の作成\n","    p = Path(imgpath)\n","    if not (p.is_dir()):\n","        p.mkdir()\n","\n","    # モデルパラメータのチェックポイントの保存先\n","    ckpt_p = Path(ckptpath)\n","    if not (ckpt_p.is_dir()):\n","        ckpt_p.mkdir()\n","\n","    config = tf.ConfigProto()\n","    config.gpu_options.allow_growth = True\n","\n","    saver = tf.train.Saver()\n","\n","    sess = tf.Session()\n","\n","    if (init):\n","        sess.run(tf.global_variables_initializer())\n","        print('Initialize')\n","\n","    ckpt = tf.train.get_checkpoint_state(str(ckpt_p.absolute()))\n","    if ckpt:\n","        # checkpointファイルから最後に保存したモデルへのパスを取得する\n","        last_model = ckpt.model_checkpoint_path\n","        print(\"load {0}\".format(last_model))\n","        # 学習済みモデルを読み込む\n","        saver.restore(sess, last_model)\n","\n","    step = len(X_train) // batch_size\n","    #step = mnist.train.num_examples // batch_size\n","\n","    # 正解ラベルのミニバッチ\n","    t1_batch = np.ones((batch_size, 1), dtype=np.float32)\n","    t0_batch = np.zeros((batch_size, 1), dtype=np.float32)\n","\n","    for epoch in range(epoch_num):\n","\n","        perm = np.random.permutation(len(X_train))\n","        # １エポックごとにかかる時間の計測\n","        start = time.time()\n","        for k in range(step):\n","            #X_batch = mnist.train.next_batch(batch_size)[0] /255.\n","            X_batch = X_train[perm][k * batch_size:(k + 1) * batch_size]\n","\n","            # Train Discrimitor\n","            # ノイズ事前分布からノイズをミニバッチ分取得\n","            noise_z = np.random.uniform(\n","                -1, 1, size=[batch_size,100]).astype(np.float32)\n","\n","            sess.run(\n","                self.opt_d,\n","                feed_dict={\n","                    self.input_X: X_batch,\n","                    self.D_is_train: True,\n","                    self.G_is_train: False,\n","                    self.gen_z: noise_z,\n","                    self.generator.keep_prob: 1.0,\n","                    self.generator.batch_size: batch_size,\n","                    self.label_t1: t1_batch,\n","                    self.label_t0: t0_batch\n","                })\n","\n","            if k % 1 == 0:\n","                # Train Generator\n","                # ノイズ事前分布からノイズをミニバッチ分取得\n","                noise_z = np.random.uniform(\n","                    -1, 1, size=[batch_size,100]).astype(np.float32)\n","                sess.run(\n","                    self.opt_g,\n","                    feed_dict={\n","                        self.gen_z: noise_z,\n","                        self.D_is_train: False,\n","                        self.G_is_train: True,\n","                        self.generator.keep_prob: 0.5,\n","                        self.generator.batch_size: batch_size,\n","                        self.label_t1: t1_batch\n","                    })\n","\n","        # 1epoch終了時の損失を表示\n","        noise_z = np.random.uniform(\n","            -1, 1, size=[batch_size,100]).astype(np.float32)\n","        train_dis_loss = sess.run(\n","            self.dis_loss,\n","            feed_dict={\n","                self.input_X: X_batch,\n","                self.D_is_train: False,\n","                self.G_is_train: False,\n","                self.gen_z: noise_z,\n","                self.generator.keep_prob: 1.0,\n","                self.generator.batch_size: batch_size,\n","                self.label_t1: t1_batch,\n","                self.label_t0: t0_batch\n","            })\n","\n","        train_gen_loss = sess.run(\n","            self.gen_loss,\n","            feed_dict={\n","                self.gen_z: noise_z,\n","                self.D_is_train: False,\n","                self.G_is_train: False,\n","                self.generator.keep_prob: 1.0,\n","                self.generator.batch_size: batch_size,\n","                self.label_t1: t1_batch\n","            })\n","        print(\n","            \"[Train] epoch: %d, dis loss: %f , gen loss : %f  Time : %f\" %\n","            (epoch, train_dis_loss, train_gen_loss, time.time() - start))\n","\n","        f = open(log_file, 'a')\n","        log_writer = csv.writer(f, lineterminator='\\n')\n","        loss_list = []\n","        loss_list.append(epoch)\n","        loss_list.append(train_dis_loss)\n","        loss_list.append(train_gen_loss)\n","        # 損失の値を書き込む\n","        log_writer.writerow(loss_list)\n","        f.close()\n","\n","        saver.save(sess, str(ckpt_p.absolute()) + '/DCGAN-mnist')\n","\n","        # 10epoch終了毎に生成モデルから1枚の画像を生成する\n","        if epoch % 2 == 0:\n","            noise_z = np.random.uniform(\n","                -1,1, size=[5,100]).astype(np.float32)\n","\n","            z_const = tf.constant(noise_z, dtype=tf.float32)\n","            gen_imgs = ((sess.run(\n","                self.generator.run(z_const, is_train=False),\n","                feed_dict={self.generator.keep_prob: 1.0,self.generator.batch_size: 5})* 0.5)+0.5)*255.\n","            for i in range(0,5):\n","                Image.fromarray(gen_imgs[i].reshape(\n","                    28, 28)).convert('RGB').save(\n","                        str(p.absolute()) +\n","                        '/generate_img_epoch{0}_{1}.jpg'.format(epoch,i))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t6sx-xdsx0lc","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}